1. **类别频率计算**：
   \[ \text{class\_freq}[i] = \frac{\text{class\_count}[i]}{\text{len(target)}} \]
   其中 \(\text{class\_count}[i]\) 是目标中类别 \(i\) 的实例数，\(\text{len(target)}\) 是目标中的总实例数。

2. **有效数目计算**：
   \[ \text{effective\_num}[i] = 1.0 - \beta^{\text{class\_freq}[i]} \]
   这里 \(\beta\) 是一个超参数，用于调整权重的分布。

3. **类别权重计算**：
   \[ \text{class\_weights}[i] = \frac{1.0 - \beta}{\text{effective\_num}[i]} \]
   这个公式应用于每个类别。

4. **调整类别权重（除第一个类别外）**：
   \[ \text{class\_weights}[i > 0] = \frac{\text{class\_weights}[i]}{1.3} \]
   这一步是为了进一步平衡类别之间的权重。

5. **带类别权重的交叉熵损失**：
   \[ \text{loss} = -\sum_{i=1}^{\text{num\_classes}} \text{class\_weights}[y] \times \log(\text{softmax}(\text{output}[i])) \]
   其中 \( y \) 是每个样本的真实类别标签（使用标签编码），\(\text{output}[i]\) 是模型对每个类别的预测输出。

以上公式体现了WeightClassBalancedLoss的核心，旨在通过对类别频率不均的数据集中每个类别的损失贡献进行调整，来解决类别不平衡问题。